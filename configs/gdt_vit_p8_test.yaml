# Configuration for the AdaptiveFocusViT (GDT-ViT) Model
# This version stops the hierarchical refinement when patch size reaches 8x8.

# A name for this specific experiment run, used for creating output directories
task_name: "gdt_vit_p8_test"

# 1. Encoder (HierarchicalViTEncoder) Configuration
encoder:
  img_size: 256 # Input image size
  embed_dim: 768
  num_heads: 12
  in_channels: 3
  stages:
    # Stage 1: Decomposes 32x32 patches into 16x16 patches
    - {depth: 4, patch_size_in: 32, patch_size_out: 16, k_selected_ratio: 0.25, max_seq_len: 64} # (256/32)^2 = 64
    # Stage 2: Decomposes 16x16 patches into 8x8 patches. This is the final stage.
    - {depth: 4, patch_size_in: 16, patch_size_out: 8,  k_selected_ratio: 0.25, max_seq_len: 64}

# 2. Classifier (DownstreamViTClassifier) Configuration
classifier:
  target_leaf_size: 16 # All leaf nodes will be resampled to this size
  embed_dim: 768 # Should generally match the encoder's output for simplicity
  depth: 6
  num_heads: 12
  num_classes: 10

# 3. Training Hyperparameters
training:
  optimizer: "AdamW"
  learning_rate: 0.0001
  weight_decay: 0.05
  scheduler: "CosineAnnealingLR"
  min_lr: 1.0e-6
  num_epochs: 120
  # Note: This is the total batch size across all GPUs for DDP training.
  # Adjust the per-device batch size based on your hardware.
  batch_size: 256