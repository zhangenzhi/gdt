# A name for this specific experiment run, used for creating output directories
task_name: "mae_imagenet_pretrain"

# 1. Model Architecture Configuration
model:
  img_size: 224
  patch_size: 2
  in_channels: 3

  # Encoder Configuration (from ViT model)
  encoder_embed_dim: 768
  encoder_depth: 12
  encoder_heads: 12
  
  # Decoder Configuration (MAE-specific)
  decoder_embed_dim: 512
  decoder_depth: 8
  decoder_heads: 16
  
  # Masking Configuration (MAE-specific)
  mask_ratio: 0.9

# 2. Training Hyperparameters
training:
  optimizer: "AdamW"
  learning_rate: 0.003
  betas: [0.9, 0.95]
  weight_decay: 0.05 # MAE paper recommends 0.05
  
  warmup_epochs: 40
  eta_min: 0.000001
  
  num_epochs: 300
  batch_size: 64

  use_compile: false
  use_fused_optimizer: false
  use_postrain: false
  gradient_accumulation_steps: 1
